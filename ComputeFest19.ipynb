{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis \n",
    "## ComputeFest 2019\n",
    "**Harvard University**<br>\n",
    "**Winter 2019**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Sentiment Analysis: First Pass\n",
    "\n",
    "This exercise is designed to help you transform and model textual data. You may find the tutorial [here](http://scikit-learn.org/stable/modules/feature_extraction.html) helpful.\n",
    "\n",
    "Your goal is to vectorize a small set of twitter data and predict the polarity (positive or negative) -- a property of the content of tweet -- of tweets using Logistic Regression.\n",
    "\n",
    "1. Load `tweet.csv`, this is a data set containing tweets and their polarity.\n",
    "\n",
    "2. Using `CountVectorizer`, turn each tweet into a vector of features. Play with the values for `min_df`, `max_df` and `stop_words`.\n",
    "\n",
    "3. Get the names of the features you produced, using `get_feature_names`.\n",
    "\n",
    "3. Split the data set into train and test (make sure there are both types of polarity present in train)\n",
    "\n",
    "4. Use Logistic Regression to predict the polarity of the test tweets. How good is your model? What is the effect of your choice of `min_df`, `max_df` and `stop_words` on your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467933112</td>\n",
       "      <td>0</td>\n",
       "      <td>the angel is going to miss the athlete this we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2323395086</td>\n",
       "      <td>0</td>\n",
       "      <td>It looks as though Shaq is getting traded to C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467968979</td>\n",
       "      <td>0</td>\n",
       "      <td>@clarianne APRIL 9TH ISN'T COMING SOON ENOUGH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990283756</td>\n",
       "      <td>0</td>\n",
       "      <td>drinking a McDonalds coffee and not understand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988884918</td>\n",
       "      <td>0</td>\n",
       "      <td>So dissapointed Taylor Swift doesnt have a Twi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  polarity                                              tweet\n",
       "0  1467933112         0  the angel is going to miss the athlete this we...\n",
       "1  2323395086         0  It looks as though Shaq is getting traded to C...\n",
       "2  1467968979         0     @clarianne APRIL 9TH ISN'T COMING SOON ENOUGH \n",
       "3  1990283756         0  drinking a McDonalds coffee and not understand...\n",
       "4  1988884918         0  So dissapointed Taylor Swift doesnt have a Twi..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweet.csv', delimiter=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Example: Process the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define documents\n",
    "corpus = ['Well done!', 'Good work, good!', 'Great effort', 'nice work', 'Excellent!', 'great work', 'nice effort', \n",
    "          'Weak', 'Poor effort!', 'not good', 'poor work', 'Could have done better on this.', 'not great', 'weak effort']\n",
    "\n",
    "# define class labels\n",
    "labels = np.array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorize text\n",
    "vectorizer = CountVectorizer(stop_words=['on', 'this'], min_df=0., max_df=1.)\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "Y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['better',\n",
       " 'could',\n",
       " 'done',\n",
       " 'effort',\n",
       " 'excellent',\n",
       " 'good',\n",
       " 'great',\n",
       " 'have',\n",
       " 'nice',\n",
       " 'not',\n",
       " 'poor',\n",
       " 'weak',\n",
       " 'well',\n",
       " 'work']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Process the tweet the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Example: Train a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.666666666667\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.4)\n",
    "\n",
    "# fit a logistic regression model to classify this data\n",
    "simple_model = LogisticRegression()\n",
    "simple_model.fit(X_train, Y_train)\n",
    "print('Train accuracy:', simple_model.score(X_train, Y_train))\n",
    "print('Test accuracy:', simple_model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Train a classifier on the tweet corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analysis\n",
    "0. Experiment with the `stop_words`, `min_df` and `max_df` parameters in `CountVectorizer`.\n",
    "1. Do the results of your classifier make sense? (**Hint:** explore the distribution of tweets in both polarity classes)\n",
    "2. What can you do to improve your model?\n",
    "3. A different way to represent documents: try using the `TfidfVectorizer` from `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Sentiment Analysis: with Neural Networks\n",
    "\n",
    "### 1. Build a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a MLP network -- play with the number of hidden layers, and the width. Start with sigmoid activation functions for the hidden nodes and for the output. \n",
    "\n",
    "***Experiment with the number of layers and observe the effect of this on the quality of the classification.***  You want to think about issues like computational effeciency and generalizability of this type of modeling. You want to compare the MLP to your logistic regression model (in terms of quality of fit, efficiency and generalizability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix a width that is suited for visualizing the output of hidden layers\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# create sequential multi-layer perceptron\n",
    "model = Sequential()\n",
    "#layer 1\n",
    "model.add(Dense(10, input_dim=input_dim, activation='sigmoid')) \n",
    "#layer 3\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "#binary classification, one output\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "# configure the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a16ea1c50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, Y_train, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.579487264156\n",
      "Train accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# evaluate the training and testing performance of your model \n",
    "# note: you should extract check both the loss function and your evaluation metric\n",
    "score = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.670864403248\n",
      "Test accuracy: 0.666666686535\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the decision boundaries (where the class probabilites are equal and thus the model is most uncertain) for most classification data sets are non-linear (the classes cannot be separated in input space by a line or a flat surface). To learn such boundaries, we typically \n",
    "\n",
    "1. apply a non-linear transformation to the input data (say by adding polynomial features)\n",
    "2. then fit a linear decision boundary (e.g. logistic regression or SVC)\n",
    "\n",
    "Here, the non-linearity of the functions represented by MLP's can help us combine the two tasks into one. That is, we don't need to preprocess the data to add non-linear features, since the MLP will learn the best non-linear transformations to the input in order to achieve the best classification. Thus, this gives us a very intuitive way to interpret the output of the hidden layers of an MLP in a classification task:\n",
    "\n",
    "> ***Outputs of each hidden layer of an MLP is a non-linear transformation of the input data into a feature space. Each hidden layer should transform the input so that it is more linearly separable.***\n",
    "\n",
    "In the following, we interpret the hidden outputs of MLP's in a classification task as mappings of our data into different feature spaces.\n",
    "\n",
    "Now let's plot the latent representations of our data given by each hidden layer.\n",
    "\n",
    "To do this, we need to extract the weights learned by our model up to layer $l$, then configure another MLP with only $l$ layers using these weights and then run our training data through the second MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the class probabilities predicted by our MLP on the training set\n",
    "Y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define another MLP with no hidden layer (only input and output) using \n",
    "# the weights up to the last hidden layer from the classifier we just learned\n",
    "latent_model = Sequential()\n",
    "latent_model.add(Dense(10, input_dim=input_dim, weights=model.layers[0].get_weights(), activation='sigmoid'))\n",
    "latent_model.add(Dense(2, weights=model.layers[1].get_weights(), activation='sigmoid'))\n",
    "activations = latent_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1732d278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the latent representation of our training data at the first hidden layer\n",
    "Y_pred = Y_pred.reshape((Y_pred.shape[0], ))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(activations[Y_pred >= 0.5, 0], activations[Y_pred >= 0.5, 1], color='r', label='Class 1')\n",
    "ax.scatter(activations[Y_pred < 0.5, 0], activations[Y_pred < 0.5, 1], label='Class 0')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Build a classifier on the tweet corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analysis\n",
    "\n",
    "1. Compare the performance of your NN classifer to the Logistic Regression model. Which model does better? Why?\n",
    "2. Can you improve your NN classifier?\n",
    "\n",
    "### 3. Train a Word Embedding for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 2], [2, 6, 2], [9, 4], [13, 6], [3], [9, 6], [13, 4], [2], [2, 4], [3, 2], [2, 6], [3, 4, 2, 6, 13, 13], [3, 9], [2, 4]]\n"
     ]
    }
   ],
   "source": [
    "#one hot encode the words\n",
    "vocab_size = 15\n",
    "encoded_corpus = [one_hot(doc, vocab_size) for doc in corpus]\n",
    "print(encoded_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  2  0  0  0  0]\n",
      " [ 2  6  2  0  0  0]\n",
      " [ 9  4  0  0  0  0]\n",
      " [13  6  0  0  0  0]\n",
      " [ 3  0  0  0  0  0]\n",
      " [ 9  6  0  0  0  0]\n",
      " [13  4  0  0  0  0]\n",
      " [ 2  0  0  0  0  0]\n",
      " [ 2  4  0  0  0  0]\n",
      " [ 3  2  0  0  0  0]\n",
      " [ 2  6  0  0  0  0]\n",
      " [ 3  4  2  6 13 13]\n",
      " [ 3  9  0  0  0  0]\n",
      " [ 2  4  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "#represent each document (made into equal length) as a sequence of words\n",
    "maxlen = 6\n",
    "padded_corpus = pad_sequences(encoded_corpus, maxlen=maxlen, padding='post')\n",
    "print(padded_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(padded_corpus, Y, test_size=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a model to find an embedding of the words that best suits the classification task\n",
    "model = Sequential()\n",
    "#embedding\n",
    "model.add(Embedding(input_dim=vocab_size,\n",
    "                    output_dim=5, \n",
    "                    input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "\n",
    "#classifier layer 1\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "#classifier layer 2\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "#binary classification, one output\n",
    "model.add(Dense(1, activation='sigmoid')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a172b87b8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])  # Compile the model\n",
    "model.fit(X_train, Y_train, epochs=500, verbose=0)  # Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.472273349762\n",
      "Train accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate the training and testing performance of your model \n",
    "# note: you should extract check both the loss function and your evaluation metric\n",
    "score = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.750593483448\n",
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Do the same for the tweet corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analysis\n",
    "\n",
    "1. Play with the output dimensions of the embedding layer and the architecture of the classifier.\n",
    "2. Compare the performance with the other models you've built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
